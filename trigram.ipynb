{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Trigram LM"
      ],
      "metadata": {
        "id": "nniV5w0DbebH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wu6uiLCqa22a"
      },
      "outputs": [],
      "source": [
        "names = open('/content/drive/MyDrive/LM/names.txt', 'r').read().splitlines()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To learn statistics, we can either use counts or a neural network. To get trigram counts:"
      ],
      "metadata": {
        "id": "Tz8PJ-8_bkFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counts = {}\n",
        "for n in names:\n",
        "  chs = ['.', '.']+list(n)+['.']\n",
        "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "    trigram = (ch1, ch2, ch3)\n",
        "    counts[trigram] = counts.get(trigram, 0) + 1"
      ],
      "metadata": {
        "id": "vYqklYCJdMc7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(counts.items(), key=lambda kv: -kv[1])[:20]"
      ],
      "metadata": {
        "id": "wp85CKIgg9XC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16aa21e9-0671-4f29-9386-07fe78ed5203"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('.', '.', 'a'), 4410),\n",
              " (('.', '.', 'k'), 2963),\n",
              " (('.', '.', 'm'), 2538),\n",
              " (('.', '.', 'j'), 2422),\n",
              " (('.', '.', 's'), 2055),\n",
              " (('a', 'h', '.'), 1714),\n",
              " (('.', '.', 'd'), 1690),\n",
              " (('n', 'a', '.'), 1673),\n",
              " (('.', '.', 'r'), 1639),\n",
              " (('.', '.', 'l'), 1572),\n",
              " (('.', '.', 'c'), 1542),\n",
              " (('.', '.', 'e'), 1531),\n",
              " (('a', 'n', '.'), 1509),\n",
              " (('o', 'n', '.'), 1503),\n",
              " (('.', 'm', 'a'), 1453),\n",
              " (('.', '.', 't'), 1308),\n",
              " (('.', '.', 'b'), 1306),\n",
              " (('.', 'j', 'a'), 1255),\n",
              " (('.', 'k', 'a'), 1254),\n",
              " (('e', 'n', '.'), 1217)]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chs = sorted(list(set(''.join(names))))\n",
        "chs.append('.') # ATTENTION: Because it is appended to the end, index for . token will be 26\n",
        "len(chs)"
      ],
      "metadata": {
        "id": "g9PzaEvQ9uiF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5128cbf9-02e2-4712-9c78-208411240f6d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ch2i = {ch:i for i, ch in enumerate(chs)}\n",
        "i2ch = {i:ch for ch, i in ch2i.items()}"
      ],
      "metadata": {
        "id": "C0y0jE8eZFyB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the trigram case, entry $N[i_{1},i_{2},i_{3}]$ is the number of times the character indexed by $i_{3}$ follows the one with index $i_{2}$ that is followed by the one indexed with $i_{1}$."
      ],
      "metadata": {
        "id": "sDBc6KBGb5JE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "N = torch.zeros((27, 27, 27), dtype=torch.int32)\n",
        "# \"training the model\"\n",
        "for n in names:\n",
        "  chs = ['.', '.']+list(n)+['.']\n",
        "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "    i_1 = ch2i[ch1]\n",
        "    i_2 = ch2i[ch2]\n",
        "    i_3 = ch2i[ch3]\n",
        "    N[i_1, i_2, i_3] += 1"
      ],
      "metadata": {
        "id": "DDtqdE-DCnCW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the trigram case, training the model means to estimate the probabilities $P(w_{i} | w_{i-1}, w_{i-2})$. Now we sample from this trigram character level language model, i.e. predict the next character given the previous two."
      ],
      "metadata": {
        "id": "BwsAYD2Fcmdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N[ch2i['.'], ch2i['.'], :]"
      ],
      "metadata": {
        "id": "2QdjsqxJr97U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6e25d01-4390-4f20-b8fc-d69b9f2aa12f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([4410, 1306, 1542, 1690, 1531,  417,  669,  874,  591, 2422, 2963, 1572,\n",
              "        2538, 1146,  394,  515,   92, 1639, 2055, 1308,   78,  376,  307,  134,\n",
              "         535,  929,    0], dtype=torch.int32)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# turn raw counts into probabilities\n",
        "p = N[ch2i['.'],ch2i['.'],:].float()\n",
        "p /= p.sum() # normalize\n",
        "p, p.sum() # p is now a probability distribution"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0k1onUr3hqV_",
        "outputId": "9243a96e-6cbc-434c-bee5-e6b38107e659"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0.1377, 0.0408, 0.0481, 0.0528, 0.0478, 0.0130, 0.0209, 0.0273, 0.0184,\n",
              "         0.0756, 0.0925, 0.0491, 0.0792, 0.0358, 0.0123, 0.0161, 0.0029, 0.0512,\n",
              "         0.0642, 0.0408, 0.0024, 0.0117, 0.0096, 0.0042, 0.0167, 0.0290, 0.0000]),\n",
              " tensor(1.))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ixs = torch.multinomial(p, num_samples=20, replacement=True) # you give me probs and I give you integers according to the probability distr\n",
        "ixs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_L6fX-njRI_",
        "outputId": "9f23d365-894c-4a84-e409-67e5c1d31f6d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([25, 13, 17,  0,  3,  1, 24,  4, 15, 14, 17, 25,  0, 11,  0,  2, 10,  0,\n",
              "         4,  3])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Broadcasting: To sum over the third dimension of a 3D tensor, use `.sum(2, keepdim=True)`. `keepdim=True` keeps the dimension alog which the summation was made, we need this for broadcasting to work properly. `keepdim=False` would squueze out the dimension which sums are collapsed, resulting in a 2D tensor in our case, then by the broadcasting rules of PyTorch, these sums would be copied over the first axis instead of third."
      ],
      "metadata": {
        "id": "ZT-Pc6_tdevk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "P = N.float()\n",
        "P /= P.sum(2, keepdim=True) # /= is an in-place op"
      ],
      "metadata": {
        "id": "J5f1ZvYNMDsV"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sampling from the trigram model\n",
        "for i in range(10):\n",
        "  ix = ch2i['.']\n",
        "  ix1 = ch2i['.']\n",
        "  out = [i2ch[ix], i2ch[ix1]]\n",
        "  while True:\n",
        "    p = P[ix,ix1]\n",
        "    ix2 = torch.multinomial(p, num_samples=1, replacement=True).item()\n",
        "    out.append(i2ch[ix2])\n",
        "    if ix2 == ch2i['.']:\n",
        "      break\n",
        "    else:\n",
        "      ix = ix1\n",
        "      ix1 = ix2\n",
        "  print(''.join(out))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOmHbhytT02q",
        "outputId": "fa0e25d4-09f1-4beb-9f20-b1759cc056bc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "..hiahraleig.\n",
            "..ca.\n",
            "..eatleild.\n",
            "..nuaazey.\n",
            "..khaleilla.\n",
            "..die.\n",
            "..abechemi.\n",
            "..marpen.\n",
            "..alinettenne.\n",
            "..tevise.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model we trained decreases the entropy, just like the living. The results look like this with no training:"
      ],
      "metadata": {
        "id": "6_u-f4G4dPdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  ix = ch2i['.']\n",
        "  ix1 = ch2i['.']\n",
        "  out = [i2ch[ix],i2ch[ix1]]\n",
        "  while True:\n",
        "    p_even = torch.ones(27) / 27.0\n",
        "    ix2 = torch.multinomial(p, num_samples=1, replacement=True).item()\n",
        "    out.append(i2ch[ix2])\n",
        "    if ix2 == ch2i['.']:\n",
        "      break\n",
        "    else:\n",
        "      ix = ix1\n",
        "      ix1 = ix2\n",
        "  print(''.join(out))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2aHn8oBgve5",
        "outputId": "3096655c-51c7-4dba-cae9-c87990b26f95"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "...\n",
            "..hl.\n",
            "..yli.\n",
            "..nrm.\n",
            "..r.\n",
            "...\n",
            "..nnnoamrhn.\n",
            "..vn.\n",
            "...\n",
            "...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log_likelihood = 0.0\n",
        "k = 0\n",
        "for n in names:\n",
        "  chs = ['.', '.']+list(n)+['.']\n",
        "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "    i_1 = ch2i[ch1]\n",
        "    i_2 = ch2i[ch2]\n",
        "    i_3 = ch2i[ch3]\n",
        "    prob = P[i_1,i_2,i_3]\n",
        "    logprob = torch.log(prob)\n",
        "    log_likelihood += logprob\n",
        "    k += 1\n",
        "    # print(f'{ch1}{ch2}{ch3}: {prob:.4f} {logprob:.4f}')\n",
        "print(f'{log_likelihood=}')\n",
        "nll = -log_likelihood\n",
        "print(f'{nll=}') # lower the better, a convenient loss metric\n",
        "print(f'{nll/k}') # average nll"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiSGyc0yXAP8",
        "outputId": "ba7e8351-67ff-4e06-952b-ca41afe1bc40"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "log_likelihood=tensor(-498647.7812)\n",
            "nll=tensor(498647.7812)\n",
            "2.185652017593384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can observe from this metric, trigram lm is an improvement over the bigram lm."
      ],
      "metadata": {
        "id": "RQCmF0iGjj-l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Neural network approach"
      ],
      "metadata": {
        "id": "ZvgzynTaj5Hc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of counting, we will learn the counts array by optimizing a set of parameters."
      ],
      "metadata": {
        "id": "yVVHO7zekfqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating the trigram dataset for training\n",
        "xs, ys = [], []\n",
        "for n in names:\n",
        "  chs = ['.', '.']+list(n)+['.']\n",
        "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "    i_1 = ch2i[ch1]\n",
        "    i_2 = ch2i[ch2]\n",
        "    i_3 = ch2i[ch3]\n",
        "    xs.append([i_1, i_2])\n",
        "    ys.append(i_3)\n",
        "\n",
        "xs = torch.tensor(xs)\n",
        "ys = torch.tensor(ys)"
      ],
      "metadata": {
        "id": "3Vf3nzWZNd3C"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywH12yBWnG0X",
        "outputId": "af666351-14ab-4c2e-eb4f-0bad6a29dfac"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([228146, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "9dstnFQZhics"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "F.one_hot(xs, num_classes=27).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03lxvviY6Azj",
        "outputId": "92e02c81-ecee-425d-a465-dbda9e997f79"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([228146, 2, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W = torch.randn((54,27), requires_grad=True)"
      ],
      "metadata": {
        "id": "tqVn-R_kOD3b"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply `W.view((27,27,27)` after training to keep the analogy between N and W."
      ],
      "metadata": {
        "id": "KXNAjWu9GEX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient descent\n",
        "for k in range(30):\n",
        "\n",
        "  # forward pass\n",
        "  xenc = F.one_hot(xs, num_classes=27).float().view((xs.shape[0],54))\n",
        "  ## softmax: takes in the logits, exponentiates them and normalizes\n",
        "  logits = xenc @ W # log-counts\n",
        "  counts = logits.exp() # equivalent to N\n",
        "  p = counts / counts.sum(1, keepdim=True)\n",
        "  loss = -p[torch.arange(xs.shape[0]), ys].log().mean() # average nll\n",
        "  print(loss.item())\n",
        "\n",
        "  # backward pass\n",
        "  W.grad = None\n",
        "  loss.backward()\n",
        "  W.grad.shape # influences of weights on the loss function\n",
        "\n",
        "  # gradient update\n",
        "  W.data += -10 * W.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBLlq0VKMsYJ",
        "outputId": "02f2ccbc-1606-462d-8806-1b163c17ed56"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.40067458152771\n",
            "2.384706497192383\n",
            "2.3812673091888428\n",
            "2.3799402713775635\n",
            "2.379206895828247\n",
            "2.378702163696289\n",
            "2.378312110900879\n",
            "2.377990961074829\n",
            "2.377715826034546\n",
            "2.3774731159210205\n",
            "2.377254009246826\n",
            "2.377051830291748\n",
            "2.376863479614258\n",
            "2.3766846656799316\n",
            "2.376514196395874\n",
            "2.376349687576294\n",
            "2.376190662384033\n",
            "2.3760361671447754\n",
            "2.375885009765625\n",
            "2.375736713409424\n",
            "2.37559175491333\n",
            "2.37544846534729\n",
            "2.375307559967041\n",
            "2.375168561935425\n",
            "2.3750312328338623\n",
            "2.3748953342437744\n",
            "2.374760627746582\n",
            "2.3746275901794434\n",
            "2.3744957447052\n",
            "2.3743646144866943\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  print(f'trigram:{i2ch[xs[i][0].item()],i2ch[xs[i][1].item()],i2ch[ys[i].item()]}')\n",
        "  print(f'output probability distribution given the input:{p[i]}')\n",
        "  print(f'probability assigned to the actual label:{p[i,ys[i]]}')\n",
        "  print(f'log likelihood:{torch.log(p[i,ys[i]])}')\n",
        "  print(f'nll:{-torch.log(p[i,ys[i]])}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHIGqFUGR0s4",
        "outputId": "2359a8a2-4a23-461d-f8d5-32fbd2e1e8d4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trigram:('.', '.', 'e')\n",
            "output probability distribution given the input:tensor([0.1358, 0.0402, 0.0479, 0.0528, 0.0480, 0.0127, 0.0204, 0.0272, 0.0189,\n",
            "        0.0748, 0.0918, 0.0491, 0.0793, 0.0356, 0.0129, 0.0155, 0.0031, 0.0511,\n",
            "        0.0640, 0.0399, 0.0047, 0.0131, 0.0102, 0.0041, 0.0170, 0.0297, 0.0003],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "probability assigned to the actual label:0.04803650826215744\n",
            "log likelihood:-3.0357940196990967\n",
            "nll:3.0357940196990967\n",
            "trigram:('.', 'e', 'm')\n",
            "output probability distribution given the input:tensor([0.0798, 0.0071, 0.0040, 0.0283, 0.0720, 0.0020, 0.0026, 0.0108, 0.0402,\n",
            "        0.0021, 0.0053, 0.2404, 0.0641, 0.0664, 0.0294, 0.0035, 0.0003, 0.1670,\n",
            "        0.0366, 0.0190, 0.0159, 0.0335, 0.0016, 0.0019, 0.0521, 0.0115, 0.0026],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "probability assigned to the actual label:0.06408719718456268\n",
            "log likelihood:-2.7475106716156006\n",
            "nll:2.7475106716156006\n",
            "trigram:('e', 'm', 'm')\n",
            "output probability distribution given the input:tensor([0.2520, 0.0068, 0.0119, 0.0049, 0.0813, 0.0006, 0.0095, 0.0046, 0.1642,\n",
            "        0.0022, 0.0073, 0.0107, 0.0404, 0.0080, 0.0458, 0.0080, 0.0056, 0.0158,\n",
            "        0.0104, 0.0146, 0.0115, 0.0065, 0.0036, 0.0040, 0.0366, 0.0027, 0.2308],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "probability assigned to the actual label:0.04039660468697548\n",
            "log likelihood:-3.2090096473693848\n",
            "nll:3.2090096473693848\n",
            "trigram:('m', 'm', 'a')\n",
            "output probability distribution given the input:tensor([0.2703, 0.0092, 0.0247, 0.0035, 0.1848, 0.0006, 0.0031, 0.0024, 0.2538,\n",
            "        0.0019, 0.0141, 0.0063, 0.0076, 0.0046, 0.0326, 0.0070, 0.0052, 0.0345,\n",
            "        0.0030, 0.0063, 0.0154, 0.0040, 0.0050, 0.0110, 0.0606, 0.0006, 0.0280],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "probability assigned to the actual label:0.2702863812446594\n",
            "log likelihood:-1.308273196220398\n",
            "nll:1.308273196220398\n",
            "trigram:('m', 'a', '.')\n",
            "output probability distribution given the input:tensor([0.0087, 0.0170, 0.0234, 0.0419, 0.0193, 0.0037, 0.0062, 0.0486, 0.0439,\n",
            "        0.0029, 0.0445, 0.1044, 0.0179, 0.1379, 0.0009, 0.0027, 0.0012, 0.2393,\n",
            "        0.0234, 0.0264, 0.0085, 0.0150, 0.0042, 0.0103, 0.0585, 0.0025, 0.0866],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "probability assigned to the actual label:0.08661842346191406\n",
            "log likelihood:-2.4462428092956543\n",
            "nll:2.4462428092956543\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A good model assigns high probabilities to data in the training set. "
      ],
      "metadata": {
        "id": "Lzosk8kyrLjp"
      }
    }
  ]
}