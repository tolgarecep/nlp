# -*- coding: utf-8 -*-
"""hod-skipgram.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15K6IHRNDbOS80wLi7VvoTYyXsE0x4WJd

Estimating embeddings in Heart of Darkness
"""

import numpy as np
import torch

import nltk
# nltk.download('punkt')
from nltk.tokenize import sent_tokenize

import re

f = open('/content/hod.txt')
text = f.read()
sentences_punc = sent_tokenize(text)

sentences_punc[-1]

sentences = [re.sub("[^A-Za-z']+", ' ', str(sentence)).lower() for sentence in sentences_punc]

tokenized = [sentence.split() for sentence in sentences]

"""Dictionaries:"""

idx2word = {}

i = 0
for sentence in tokenized:
  for token in sentence:
    if token not in idx2word.values():
      idx2word[i] = token
      i += 1
vocab_size = len(idx2word)

word2idx = {word:id for id, word in idx2word.items()}
word2idx['horror']

"""(center_word, context_word) pairs:"""

w_size = 2 # context window size

X = [] # dataset
for sentence in tokenized:
  indices = [word2idx[word] for word in sentence]
  for pos_center in range(len(indices)): # position of the center word in the sentence
    for w in range(-w_size, w_size + 1):
      pos_context = pos_center + w
      if pos_context < 0 or pos_context >= len(indices) or pos_center == pos_context:
        continue
      i_context = indices[pos_context]
      i_center = indices[pos_center]
      X.append((i_center, i_context))
X = np.array(X)

"""Skip-gram:"""

def skip_gram_pass(pair, lr, epoch): # forward pass, backpropagation and gradient descent update
  # input
  i_center = pair[0]
  x = torch.zeros(vocab_size).double()
  x[i_center] = 1.0  
  # ground truth
  i_context = pair[1]
  y_true = torch.tensor([i_context]).long()

  # embedding
  embedding_dims = 5
  W_1 = torch.rand((embedding_dims, vocab_size), dtype=torch.float64, requires_grad=True)
  z_1 = torch.matmul(W_1, x)

  # projection
  W_2 = torch.rand((vocab_size, embedding_dims), dtype=torch.float64, requires_grad=True)
  z_2 = torch.matmul(W_2, z_1)

  log_softmax = torch.nn.functional.log_softmax(z_2, dim=0)
  loss = torch.nn.functional.nll_loss(log_softmax.view(1,-1), y_true)

  loss.backward()

  # gradient descent update
  W_1.data -= lr * W_1.grad.data
  W_2.data -= lr * W_2.grad.data

  W_1.grad.data.zero_()
  W_2.grad.data.zero_()
  
  return W_1, loss # learned embeddings

num_epochs = 100
lr = 0.01

for epoch in range(num_epochs):
  for pair in X:
    emb, loss = skip_gram_pass(pair, lr, epoch)
  if epoch % 10 == 0:
    print(f'Loss at epo {epoch}: {loss}')
  if epoch == num_epochs-1:
      optimized_embeddings = emb